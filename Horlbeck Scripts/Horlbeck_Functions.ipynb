{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b020913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import sqlalchemy\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from scipy import optimize\n",
    "from scipy.stats import sem\n",
    "import subprocess\n",
    "\n",
    "def check_repeated_constructs(x, index_loc):\n",
    "    '''\n",
    "    Helper function, Returns location of counts with respect to study conditions/replicate names.\n",
    "    '''\n",
    "    if len(x) < max(index_loc):\n",
    "        sub = index_loc[index_loc < len(x)]\n",
    "        return(x[sub])\n",
    "    else:\n",
    "        return(x[index_loc])\n",
    "    \n",
    "def get_raw_counts(curr_counts):\n",
    "    #display(curr_counts) \n",
    "    '''\n",
    "    Helper function, gets the raw counts based on the T0 and TEnd annotations of the sample names\n",
    "    '''\n",
    "    print('Getting raw counts...')\n",
    "    # get counts\n",
    "    T0_counts = curr_counts['T0_counts'].apply(    \n",
    "        lambda x: np.array(x.split(\";\"), dtype = np.float64)\n",
    "    )\n",
    "\n",
    "    T0_counts = pd.DataFrame(data = T0_counts.tolist(),\n",
    "                   index = T0_counts.index, columns = curr_counts['T0_replicate_names'].iloc[0].split(';'))\n",
    "\n",
    "    TEnd_counts = curr_counts['TEnd_counts'].apply(    \n",
    "        lambda x: np.array(x.split(\";\"), dtype = np.float64)\n",
    "    )\n",
    "\n",
    "    TEnd_counts = pd.DataFrame(data = TEnd_counts.tolist(),\n",
    "                   index = TEnd_counts.index, columns = curr_counts['TEnd_replicate_names'].iloc[0].split(';'))\n",
    "    \n",
    "    # make sure no columns are filled with NAs completely (in case of additional annotations)\n",
    "    NA_replicate = T0_counts.isna().sum()\n",
    "    if (NA_replicate == T0_counts.shape[0]).sum() > 0:\n",
    "        print('Removing NA replicate from T0...')\n",
    "        T0_counts.drop(NA_replicate.index[NA_replicate == T0_counts.shape[0]], axis = 1, inplace = True)\n",
    "    \n",
    "    NA_replicate = TEnd_counts.isna().sum()\n",
    "    if (NA_replicate == TEnd_counts.shape[0]).sum() > 0:\n",
    "        print('Removing NA replicate from TEnd...')\n",
    "        TEnd_counts.drop(NA_replicate.index[NA_replicate == TEnd_counts.shape[0]], axis = 1, inplace = True)\n",
    "\n",
    "    T0_counts = T0_counts.fillna(0)\n",
    "    TEnd_counts = TEnd_counts.fillna(0)\n",
    "    \n",
    "    return((T0_counts, TEnd_counts))\n",
    "\n",
    "def run_horlbeck_score(curr_counts, curr_study, curr_cl, do_preprocessing = True, store_loc = os.getcwd(), save_dir = 'HORLBECK_Files', re_run = False, filterThreshold = 35):\n",
    "    '''\n",
    "    \n",
    "    Calculates Horlbeck score. Score files will created at the designated store location and save directory. \n",
    "\n",
    "    **Params**:\n",
    "    * curr_counts: Counts to calculate scores to.)\n",
    "    * curr_study: String, name of study to analyze data for.\n",
    "    * curr_cl: String, name of cell line to analyze data for.\n",
    "    * store_loc: String: Directory to store the MAGeCK files to. (Default: current working directory)\n",
    "    * save_dir: String: Folder name to store the MAGeCK files to. (Default: 'Horlbeck_Files')\n",
    "    * do_preprocessing: Boolean. Run Horlbeck preprocessing (Default: True)\n",
    "    * re_run: Boolean. Recreate and rerun the results instead of loading for subsequent analyses (Default: False)\n",
    "\n",
    "    **Returns**:\n",
    "\n",
    "    * horlbeck_res: A dict that contains a pandas dataframe for Horlbeck Score.\n",
    "\n",
    "    '''\n",
    "    print('Running horlbeck score...')\n",
    "    \n",
    "    ######### preprocessing\n",
    "    \n",
    "    print('Running preprocessing...')\n",
    "\n",
    "    if do_preprocessing:\n",
    "        curr_counts = run_horlbeck_preprocessing(curr_counts, filterThreshold)\n",
    "\n",
    "\n",
    "    #########/ preprocessing\n",
    "    \n",
    "    # get save location\n",
    "    save_loc = os.path.join(store_loc, save_dir, curr_study, curr_cl)\n",
    "    os.makedirs(save_loc, exist_ok = True)\n",
    "    \n",
    "    ######### original horlbeck scoring\n",
    "\n",
    "    # first, drop the rows with nan replicateFCname\n",
    "    curr_counts.dropna(subset = ['FC_Averaged_abbaAveraged'], inplace = True)\n",
    "\n",
    "    # get ab/ba\n",
    "    a_average, b_average = curr_counts.loc[curr_counts['target_type'] != 'Dual'].copy(), curr_counts.loc[curr_counts['target_type'] != 'Dual'].copy()\n",
    "    #curr_counts.loc[curr_counts['target_type'] == 'Single'].copy(), curr_counts.loc[curr_counts['target_type'] == 'Single'].copy()\n",
    "    #\n",
    "    a_average = a_average[a_average['sgRNA_target_name_g2'] == \"CONTROL\"]\n",
    "    b_average = b_average[b_average['sgRNA_target_name_g1'] == \"CONTROL\"]\n",
    "\n",
    "    a_average = a_average.groupby('sgRNA_guide_name_g1')['FC_Averaged_abbaAveraged'].apply(np.mean)\n",
    "    b_average = b_average.groupby('sgRNA_guide_name_g2')['FC_Averaged_abbaAveraged'].apply(np.mean)\n",
    "\n",
    "    # single, control, and dual phenotypes are used in calculation\n",
    "    all_pairs = set(curr_counts['sgRNA_guide_name_g1']).union(set(curr_counts['sgRNA_guide_name_g2']))\n",
    "    curr_counts['GI_Averaged'] = 0\n",
    "\n",
    "    # for missing pairs, update a_average, b_average\n",
    "    a_average_0s = list(all_pairs.difference(set(a_average.index)))\n",
    "    a_average = pd.concat([a_average, pd.Series(data = np.zeros(len(a_average_0s)), index = a_average_0s)])\n",
    "\n",
    "    b_average_0s = list(all_pairs.difference(set(b_average.index)))\n",
    "    b_average = pd.concat([b_average, pd.Series(data = np.zeros(len(b_average_0s)), index = b_average_0s)])\n",
    "\n",
    "    # store in a matrix\n",
    "    GI_Score_1 = pd.DataFrame(0, index = sorted(list(all_pairs)), columns = sorted(list(all_pairs)), dtype = np.float64)\n",
    "    GI_Score_2 = pd.DataFrame(0, index = sorted(list(all_pairs)), columns = sorted(list(all_pairs)), dtype = np.float64)\n",
    "    \n",
    "    \n",
    "    # scores have already been computed\n",
    "    if os.path.exists(os.path.join(save_loc, \"GI_Score_1.gzip\")) and (not re_run):\n",
    "        print('Scores exist For GI_Score_1! Loading...')\n",
    "        GI_Score_1 = pd.read_pickle(os.path.join(save_loc, \"GI_Score_1.gzip\"))\n",
    "    else:\n",
    "        print('Calculating GI_Score_1...')\n",
    "        \n",
    "        ## A orientation ()\n",
    "\n",
    "        ## go through all query sgRNAs\n",
    "        for query_sgRNA in all_pairs:\n",
    "\n",
    "            ## get all the pairs with the given query\n",
    "            idx_loc = (curr_counts['sgRNA_guide_name_g2'] == query_sgRNA)\n",
    "            #print(query_sgRNA)\n",
    "            #print(idx_loc)\n",
    "\n",
    "            if len(idx_loc) == 0:\n",
    "                continue\n",
    "\n",
    "            ## all pairs \n",
    "            curr_filtered_pairs = curr_counts.loc[idx_loc, :]\n",
    "\n",
    "            ## get sgRNAs assayed together with the query sgRNA\n",
    "            selected_sgRNAs = curr_filtered_pairs['sgRNA_guide_name_g1'].values\n",
    "\n",
    "            if 'Control' in set(curr_counts['target_type']):\n",
    "                #print(\"In Control\")\n",
    "                control_sgRNAs = np.where(curr_filtered_pairs['sgRNA_target_name_g1'] == \"CONTROL\")[0]\n",
    "                #print(control_sgRNAs)\n",
    "                #print(len(control_sgRNAs))\n",
    "\n",
    "            # Fit to a quadratic formula, where the x is the single phenotypes and y is the pair phenotypes\n",
    "\n",
    "            xs = a_average.loc[selected_sgRNAs].values # a -> b\n",
    "            ys = curr_filtered_pairs['FC_Averaged_abbaAveraged'].values\n",
    "            bs = b_average.loc[query_sgRNA] # b -> a\n",
    "\n",
    "            res_fn = quadFitForceIntercept(xs, ys, bs)\n",
    "\n",
    "            # get expected\n",
    "            expected_phenotype = res_fn(xs)\n",
    "\n",
    "            # the difference is the GI score\n",
    "            GI_Score = ys - expected_phenotype\n",
    "\n",
    "            if ('Control' in set(curr_counts['target_type'])) and len(control_sgRNAs) > 0:\n",
    "                if GI_Score[control_sgRNAs].std() != 0:\n",
    "                    GI_Score /= GI_Score[control_sgRNAs].std()\n",
    "                   \n",
    "           \n",
    "            #print(GI_Score_1.loc[query_sgRNA, selected_sgRNAs].shape)\n",
    "            #print(len(GI_Score))\n",
    "            GI_Score = GI_Score.astype('int32')\n",
    "            GI_Score_1.loc[query_sgRNA, selected_sgRNAs] = GI_Score\n",
    "            #print(GI_Score_1.loc[query_sgRNA, selected_sgRNAs])\n",
    "        # save scores for future loading\n",
    "      #  GI_Score_1.to_pickle(os.path.join(save_loc, \"GI_Score_1.gzip\"))\n",
    "    \n",
    "    if os.path.exists(os.path.join(save_loc, \"GI_Score_2.gzip\")) and (not re_run):\n",
    "        print('Scores exist For GI_Score_2! Loading...')\n",
    "        GI_Score_2 = pd.read_pickle(os.path.join(save_loc, \"GI_Score_2.gzip\"))\n",
    "    else:\n",
    "        print('Calculating GI_Score_2...')\n",
    "\n",
    "        ## B orientation ()\n",
    "        ## go through all query sgRNAs\n",
    "        for query_sgRNA in all_pairs:\n",
    "\n",
    "            ## get all the pairs with the given query\n",
    "            idx_loc = (curr_counts['sgRNA_guide_name_g1'] == query_sgRNA)\n",
    "\n",
    "            if len(idx_loc) == 0:\n",
    "                continue\n",
    "\n",
    "            ## all pairs \n",
    "            curr_filtered_pairs = curr_counts.loc[idx_loc, :]\n",
    "\n",
    "            ## get sgRNAs assayed together with the query sgRNA\n",
    "            selected_sgRNAs = curr_filtered_pairs['sgRNA_guide_name_g2'].values\n",
    "\n",
    "            if 'Control' in set(curr_counts['target_type']):\n",
    "                control_sgRNAs = np.where(curr_filtered_pairs['sgRNA_target_name_g2'] == \"CONTROL\")[0]\n",
    "\n",
    "            # Fit to a quadratic formula, where the x is the single phenotypes and y is the pair phenotypes\n",
    "\n",
    "            xs = b_average.loc[selected_sgRNAs].values # b -> a\n",
    "            ys = curr_filtered_pairs['FC_Averaged_abbaAveraged'].values\n",
    "            bs = a_average.loc[query_sgRNA] # a -> b\n",
    "\n",
    "            res_fn = quadFitForceIntercept(xs, ys, bs)\n",
    "\n",
    "            # get expected\n",
    "            expected_phenotype = res_fn(xs)\n",
    "\n",
    "            # the difference is the GI score\n",
    "            GI_Score = ys - expected_phenotype\n",
    "\n",
    "            if ('Control' in set(curr_counts['target_type'])) and len(control_sgRNAs) > 0:\n",
    "                if GI_Score[control_sgRNAs].std() != 0:\n",
    "                    GI_Score /= GI_Score[control_sgRNAs].std()\n",
    "\n",
    "            # set the \n",
    "            #curr_res['sgRNA_level']['dual'].loc[idx_loc, replicate_GI_name] += GI_Score\n",
    "            GI_Score_2.loc[query_sgRNA, selected_sgRNAs] = GI_Score\n",
    "\n",
    "\n",
    "        # save scores for future loading\n",
    "        GI_Score_2.to_pickle(os.path.join(save_loc, \"GI_Score_2.gzip\"))\n",
    "    \n",
    "    \n",
    "    # average between A and B orientations\n",
    "    #curr_res['sgRNA_level']['dual'][replicate_GI_name] /= 2\n",
    "    GI_Score_avg = (GI_Score_1 + GI_Score_2)/2\n",
    "    GI_Score_avg = (GI_Score_avg + GI_Score_avg.T)/2\n",
    "\n",
    "    for i in range(len(curr_counts['GI_Averaged'])):\n",
    "        guide_1 = curr_counts['sgRNA_guide_name_g1'].iloc[i]\n",
    "        guide_2 = curr_counts['sgRNA_guide_name_g2'].iloc[i]\n",
    "\n",
    "        curr_counts['GI_Averaged'].iloc[i] = GI_Score_avg.loc[guide_1, guide_2]\n",
    "\n",
    "    \n",
    "    ######### /original horlbeck scoring\n",
    "    \n",
    "    \n",
    "    # store results\n",
    "    SL_score = curr_counts.groupby('gene_pair')['GI_Averaged'].apply(lambda x: np.mean(x))\n",
    "    SE = curr_counts.groupby('gene_pair')['GI_Averaged'].apply(lambda x: sem(x, ddof=1))\n",
    "\n",
    "    genes_1 = [i.split('|')[0] for i in SL_score.index]\n",
    "    genes_2 = [i.split('|')[1] for i in SL_score.index]\n",
    "    \n",
    "    horlbeck_results = pd.DataFrame(data = {'SL_score' : SL_score.values,\n",
    "                                             'standard_error' : SE.values,\n",
    "                                             'Gene 1' : genes_1,\n",
    "                                             'Gene 2' : genes_2}, index = SL_score.index)\n",
    "    \n",
    "    \n",
    "    # remove possible controls\n",
    "    control_idx = np.array([True if 'CONTROL' in i else False for i in horlbeck_results.index])\n",
    "    horlbeck_results = horlbeck_results.loc[~control_idx]\n",
    "    \n",
    "    results = {}\n",
    "    results['HORLBECK_SCORE'] = horlbeck_results\n",
    "\n",
    "    return(results)\n",
    "\n",
    "def quadFitForceIntercept(xdata, ydata, bdata):\n",
    "    m1 = optimize.fmin(lambda m, x, y: ((m[0]*(x**2) + m[1]*x + bdata - y)**2).sum(), x0=[0.1,0.1], args=(xdata, ydata), disp=0)\n",
    "    \n",
    "    return lambda x1: m1[0]*(np.array(x1)**2) + m1[1]*np.array(x1) + bdata\n",
    "\n",
    "\n",
    "def run_horlbeck_preprocessing(curr_counts, filterThreshold = 35, pseudocount = 10):\n",
    "        \n",
    "    T0_counts, TEnd_counts = get_raw_counts(curr_counts.copy())\n",
    "    \n",
    "    # horlbeck uses single x single as double, proceed to move them to dual instead\n",
    "    replace_idx = (curr_counts['target_type'] == 'Single') & (curr_counts['sgRNA_target_name_g1'] == curr_counts['sgRNA_target_name_g2'])\n",
    "    curr_counts.loc[replace_idx, 'target_type'] = 'Dual'\n",
    "\n",
    "    if T0_counts.shape[1] != TEnd_counts.shape[1]:\n",
    "        print(\"Mismatch times, averaging...\")\n",
    "\n",
    "        T0_counts = pd.DataFrame(data = T0_counts.apply(lambda x: np.mean(x), axis = 1).values,\n",
    "                             index = T0_counts.index)\n",
    "\n",
    "        TEnd_counts = pd.DataFrame(data = TEnd_counts.apply(lambda x: np.mean(x), axis = 1).values,\n",
    "                 index = TEnd_counts.index)\n",
    "\n",
    "    T0_counts = pd.concat([T0_counts, curr_counts['sgRNA_guide_name_g1'], curr_counts['sgRNA_guide_name_g2']], axis = 1)\n",
    "    TEnd_counts = pd.concat([TEnd_counts, curr_counts['sgRNA_guide_name_g1'], curr_counts['sgRNA_guide_name_g2']], axis = 1)\n",
    "    all_sgRNAs = set(TEnd_counts['sgRNA_guide_name_g1']).union(set(TEnd_counts['sgRNA_guide_name_g2']))\n",
    "\n",
    "    # add sorted targets\n",
    "    sorted_gene_pairs, sorted_gene_guides = sort_pairs_and_guides(curr_counts.copy())\n",
    "    curr_counts['sgRNA_pair'] = sorted_gene_guides\n",
    "    curr_counts['gene_pair'] = sorted_gene_pairs\n",
    "    \n",
    "    replicate_list = []\n",
    "    for replicate_i in range(len(T0_counts.columns)-2):\n",
    "        print(\"For replicate \" + str(replicate_i + 1))\n",
    "        meanCounts = pd.concat((TEnd_counts.iloc[:,replicate_i].groupby(TEnd_counts['sgRNA_guide_name_g1']).agg(np.median),TEnd_counts.iloc[:,replicate_i].groupby(TEnd_counts['sgRNA_guide_name_g2']).agg(np.median)),axis=1, keys=['sgRNA_guide_name_g1', 'sgRNA_guide_name_g2'])\n",
    "        sgsToFilter = set(meanCounts.loc[meanCounts.loc[:,'sgRNA_guide_name_g1'] < filterThreshold].index).union(set(meanCounts.loc[meanCounts.loc[:,'sgRNA_guide_name_g2'] < filterThreshold].index))\n",
    "        print(\" \".join([\"Total of\", str(len(sgsToFilter)), 'sgRNAs were filtered out of', str(len(all_sgRNAs))]))\n",
    "\n",
    "        chosen_idx = np.array([True if i not in sgsToFilter else False for i in TEnd_counts['sgRNA_guide_name_g1']]) & np.array([True if i not in sgsToFilter else False for i in TEnd_counts['sgRNA_guide_name_g2']])\n",
    "        TEnd_counts_curr = TEnd_counts.iloc[chosen_idx, replicate_i]\n",
    "        T0_counts_curr = T0_counts.iloc[chosen_idx, replicate_i]\n",
    "\n",
    "        counts_ratio = ((T0_counts_curr + pseudocount).sum()*1.0)/(TEnd_counts_curr + pseudocount).sum()\n",
    "\n",
    "        # calculate FC like in horlbeck\n",
    "        replicate_FC = np.log2((TEnd_counts_curr + pseudocount)/(T0_counts_curr + pseudocount)/counts_ratio)\n",
    "        replicate_FC.columns = ['Replicate_' + str(replicate_i+1) + \"_FC\"]\n",
    "        replicate_FC.name = 'Replicate_' + str(replicate_i+1) + \"_FC\"\n",
    "\n",
    "        # get control\n",
    "        control_effect = 0\n",
    "        if 'Control' in set(curr_counts['target_type']):\n",
    "            control_index = curr_counts['target_type'] == 'Control'\n",
    "            if control_index.sum() != 0:\n",
    "                control_effect = replicate_FC.loc[control_index].median()\n",
    "\n",
    "        replicate_FC -= control_effect\n",
    "\n",
    "        # doubling differences, taken from original code\n",
    "        replicate_FC /= 6.3\n",
    "\n",
    "        curr_counts = curr_counts.join(replicate_FC)\n",
    "\n",
    "        replicate_list.append(replicate_FC)\n",
    "\n",
    "    # save the results to original data\n",
    "    replicate_list = pd.concat(replicate_list, axis = 1)\n",
    "    replicate_list = replicate_list.dropna()\n",
    "\n",
    "    replicate_list = replicate_list.mean(axis = 1)\n",
    "\n",
    "    replicate_list.columns = ['FC_Averaged']\n",
    "    replicate_list.name = 'FC_Averaged'\n",
    "\n",
    "    curr_counts = curr_counts.join(replicate_list)\n",
    "\n",
    "    average_of_transpose = curr_counts.groupby('sgRNA_pair')['FC_Averaged'].apply(np.nanmean)\n",
    "    curr_counts = curr_counts.join(average_of_transpose,\n",
    "                             on = 'sgRNA_pair',\n",
    "                             rsuffix = \"_abbaAveraged\")\n",
    "    \n",
    "    return(curr_counts)\n",
    "\n",
    "\n",
    "def prepare_study_for_export(sequence_ref, counts_ref, score_ref, study_controls = None, study_conditions = None, can_control_be_substring = True, remove_unrelated_counts = False):\n",
    "    '''\n",
    "        \n",
    "    Prepares the counts, scores, and sequences files for insertion into the DB.\n",
    "\n",
    "    **Params**:\n",
    "\n",
    "    * score_ref: A pandas table that adheres to the scores table template. \n",
    "    * sequence_ref: A pandas table that adheres to the sequence table template (default: None). \n",
    "    * counts_ref: A pandas table that adheres to the counts table template (default: None). \n",
    "    * study_controls: A list of control targets of the sgRNAs (default: None).\n",
    "    * study_conditions: A list of two lists; first list contains the replicate names of initial time point, and second list contains the same for final time point (default: None).\n",
    "    * can_control_be_substring: Can the controls be a substring of gene targets (in case of possible name conventions: default: True)\n",
    "    * remove_unrelated_counts = Remove dual counts with targets that are outside of supplied scores targets? (default: False)\n",
    "\n",
    "    **Returns**:\n",
    "\n",
    "    * A dictionary of three items:\n",
    "        * scores_ref: Contains the procesed scores table (if supplied)\n",
    "        * sequences_ref: Contains the procesed sequences table (if supplied)\n",
    "        * counts_ref: Contains the procesed counts table (if supplied)\n",
    "    '''\n",
    "    ## make sure the columns are within each table, if not return error\n",
    "    sequence_ref_needed_columns = {'sgRNA_guide_name', 'sgRNA_guide_seq', 'sgRNA_target_name'}\n",
    "    \n",
    "    if sequence_ref is not None:\n",
    "        if len(sequence_ref_needed_columns.difference(sequence_ref.columns)) > 0:\n",
    "            print('Error')\n",
    "            print('sequence_ref')\n",
    "            return\n",
    "        # reset index by default\n",
    "        sequence_ref.sort_values('sgRNA_target_name', ignore_index = True, inplace = True)\n",
    "        sequence_ref.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    counts_ref_needed_columns = {'guide_1', 'guide_2', 'gene_1', 'gene_2', 'count_replicates', 'cell_line_origin', 'study_origin', 'study_conditions'}\n",
    "    if counts_ref is not None:\n",
    "        if len(counts_ref_needed_columns.difference(counts_ref.columns)) > 0:\n",
    "            print('Error')\n",
    "            print('counts_ref')\n",
    "            return\n",
    "        # reset index by default\n",
    "        counts_ref.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    score_ref_needed_columns = {'gene_1', 'gene_2', 'study_origin', 'cell_line_origin', 'SL_score', 'SL_score_cutoff', 'statistical_score', 'statistical_score_cutoff'}\n",
    "    if (score_ref is None) and (counts_ref is not None):\n",
    "        print('There are no scores, but there are counts...Generating Placeholder...')\n",
    "        score_ref = create_placeholder_scores(counts_ref.copy(), sequence_ref.copy())\n",
    "    if len(score_ref_needed_columns.difference(score_ref.columns)) > 0:\n",
    "        print('Error')\n",
    "        print('score_ref')\n",
    "        return\n",
    "    # reset index by default\n",
    "    score_ref.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    if study_controls is not None:\n",
    "        study_controls = [i.upper() for i in study_controls]\n",
    "    \n",
    "    \n",
    "    ## prepare each table to be inserted to their respective tables\n",
    "    \n",
    "    print(\"Starting processing...\")\n",
    "    \n",
    "    ################################# first, handle the scores ref\n",
    "    print('Score reference...')\n",
    "    \n",
    "    # fill NA\n",
    "    score_ref = score_ref.fillna(0)\n",
    "    \n",
    "    for col in ['gene_1', 'gene_2', 'cell_line_origin']:\n",
    "        score_ref[col] = [i.upper() for i in score_ref[col]]\n",
    "    \n",
    "    genes_A = score_ref['gene_1'].values\n",
    "    genes_B = score_ref['gene_2'].values\n",
    "    sorted_genes = []\n",
    "    for i in range(score_ref.shape[0]):\n",
    "        sorted_genes.append('_'.join(sorted([genes_A[i], genes_B[i]])))\n",
    "    score_ref[\"gene_pair\"] = sorted_genes\n",
    "\n",
    "    # remove same ones\n",
    "    score_ref = score_ref.loc[~(score_ref[\"gene_1\"].values == score_ref[\"gene_2\"].values)]\n",
    "\n",
    "    # remove controls from SL scores\n",
    "    if study_controls is not None:\n",
    "        control_idx = np.array([False] * score_ref.shape[0])\n",
    "\n",
    "        for curr_control in study_controls:\n",
    "            \n",
    "            if can_control_be_substring:\n",
    "                control_idx = control_idx | np.array([True if curr_control in i else False for i in score_ref[\"gene_1\"]]) | np.array([True if curr_control in i else False for i in score_ref[\"gene_2\"]])\n",
    "                \n",
    "            control_idx = control_idx | np.array([True if i in curr_control else False for i in score_ref[\"gene_1\"]]) | np.array([True if i in curr_control else False for i in score_ref[\"gene_2\"]])\n",
    "\n",
    "        print('Controls within SL score that are removed: ')\n",
    "        print(control_idx.sum())\n",
    "        print('---')\n",
    "\n",
    "        score_ref = score_ref.loc[~control_idx]\n",
    "        \n",
    "    if (score_ref['statistical_score_cutoff'].iloc[0] != 0) and (score_ref['SL_score_cutoff'].iloc[0] != 0):\n",
    "        print('Both GI and Stat cutoffs are present...')\n",
    "        score_ref['SL_or_not'] = (score_ref['SL_score'] <= (score_ref['SL_score_cutoff'].iloc[0])) & (score_ref['statistical_score'] <= (score_ref['statistical_score_cutoff'].iloc[0]))\n",
    "    elif score_ref['SL_score_cutoff'].iloc[0] != 0:\n",
    "        print('Only GI cutoff is present...')\n",
    "        score_ref['SL_or_not'] = score_ref['SL_score'] <= score_ref['SL_score_cutoff'].iloc[0]\n",
    "    elif score_ref['statistical_score'].iloc[0] != 0:\n",
    "        print('Only Stat cutoff is present...')\n",
    "        score_ref['SL_or_not'] = score_ref['statistical_score'] <= score_ref['statistical_score_cutoff'].iloc[0]\n",
    "    else:\n",
    "        print('No scores/stats cutoffs are available, possibly generated. Setting all to be NOT SL')\n",
    "        score_ref['SL_or_not'] = [False] * score_ref.shape[0]\n",
    "        \n",
    "    score_ref.loc[score_ref['SL_or_not'], 'SL_or_not'] = 'SL'\n",
    "    score_ref.loc[score_ref['SL_or_not'] != 'SL', 'SL_or_not'] = 'Not SL'\n",
    "    \n",
    "    ################################# score ref - DONE\n",
    "    \n",
    "    print('Counts reference...')\n",
    "    \n",
    "    if counts_ref is not None:\n",
    "\n",
    "        for col in ['guide_1', 'guide_2', 'gene_1', 'gene_2', 'cell_line_origin']:\n",
    "            counts_ref[col] = [i.upper() for i in counts_ref[col]]\n",
    "\n",
    "\n",
    "        # label whether single, double, or control\n",
    "        sgRNA_true_pair_index = np.array([i for i in range(counts_ref.shape[0]) if (str(counts_ref[\"gene_1\"].iloc[i]) not in study_controls) and (str(counts_ref[\"gene_2\"].iloc[i]) not in study_controls) and (str(counts_ref[\"gene_1\"].iloc[i]) != str(counts_ref[\"gene_2\"].iloc[i]))])\n",
    "        print(' '.join([\"Number of double pairs:\", str(len(sgRNA_true_pair_index))]))\n",
    "\n",
    "        sgRNA_control_pair_index = np.array([i for i in range(counts_ref.shape[0]) if (str(counts_ref[\"gene_1\"].iloc[i]) in study_controls) and (str(counts_ref[\"gene_2\"].iloc[i]) in study_controls)])\n",
    "        print(' '.join([\"Number of controls:\", str(len(sgRNA_control_pair_index))]))\n",
    "\n",
    "        sgRNA_single_gene_index = np.array(sorted(list(set(range(counts_ref.shape[0])).difference(set(np.concatenate((sgRNA_true_pair_index, sgRNA_control_pair_index)))))))\n",
    "        print(' '.join([\"Number of singles:\", str(len(sgRNA_single_gene_index))]))\n",
    "\n",
    "        if (len(sgRNA_single_gene_index) + len(sgRNA_control_pair_index) + len(sgRNA_true_pair_index)) != counts_ref.shape[0]:\n",
    "            print('Missing annotation')\n",
    "            print(counts_ref.shape[0] - (len(sgRNA_single_gene_index) + len(sgRNA_control_pair_index) + len(sgRNA_true_pair_index)))\n",
    "\n",
    "        counts_ref['target_type'] = 'N/A'\n",
    "        counts_ref['target_type'].iloc[sgRNA_true_pair_index] = 'Dual'\n",
    "        counts_ref['target_type'].iloc[sgRNA_control_pair_index] = 'Control'\n",
    "        counts_ref['target_type'].iloc[sgRNA_single_gene_index] = 'Single'\n",
    "\n",
    "\n",
    "        if 'Type' in counts_ref.columns:\n",
    "            counts_ref = counts_ref.drop(columns = ['Type'])\n",
    "        if 'Sequencing' in counts_ref.columns:\n",
    "            counts_ref = counts_ref.drop(columns = ['Sequencing'])\n",
    "\n",
    "\n",
    "        ## seperate the replicate counts across T0 and TEnd\n",
    "        counts_ref['T0_counts'] = \"\"\n",
    "        counts_ref['T0_replicate_names'] = \"\"\n",
    "        counts_ref['TEnd_counts'] = \"\"\n",
    "        counts_ref['TEnd_replicate_names'] = \"\"\n",
    "\n",
    "        if isinstance(study_conditions, dict):\n",
    "            # for different cell_line_origins within a study\n",
    "\n",
    "            for cell_line_origin in study_conditions:\n",
    "                curr_conditions = study_conditions[cell_line_origin]\n",
    "\n",
    "                # access the cell_line_origin counts\n",
    "                access_level = counts_ref.loc[counts_ref['cell_line_origin'] == cell_line_origin].copy()\n",
    "\n",
    "                ## get all conditions\n",
    "                condition = access_level['study_conditions'].value_counts().index.tolist()\n",
    "                condition = condition[0].split(';')\n",
    "\n",
    "                # time point T_0\n",
    "                t_0_index = np.array([i for i in range(len(condition)) if condition[i] in curr_conditions[0]])\n",
    "                # time point T_end\n",
    "                t_end_index = np.array([i for i in range(len(condition)) if condition[i] in curr_conditions[1]])\n",
    "\n",
    "                # get counts\n",
    "                replicate_sep =  access_level[\"count_replicates\"].apply(    \n",
    "                    lambda x: np.array(x.split(\";\"), dtype = np.float64)\n",
    "                )\n",
    "\n",
    "                # get time point \n",
    "                t_0_comb = replicate_sep.apply(\n",
    "                    lambda x: check_repeated_constructs(x, t_0_index)\n",
    "                ).apply(lambda x: ';'.join(x.astype(np.str_)))\n",
    "\n",
    "                t_end_comb = replicate_sep.apply(\n",
    "                    lambda x: check_repeated_constructs(x, t_end_index)#np.median(x[t_end_index])\n",
    "                ).apply(lambda x: ';'.join(x.astype(np.str_)))\n",
    "\n",
    "                access_level['T0_counts'] = t_0_comb\n",
    "                access_level['T0_replicate_names'] = ';'.join(curr_conditions[0])\n",
    "                access_level['TEnd_counts'] = t_end_comb\n",
    "                access_level['TEnd_replicate_names'] = ';'.join(curr_conditions[1])\n",
    "\n",
    "                counts_ref.loc[counts_ref['cell_line_origin'] == cell_line_origin] = access_level\n",
    "        else:\n",
    "            # for only one cell_line_origin\n",
    "            curr_conditions = study_conditions\n",
    "\n",
    "            ## get all conditions\n",
    "            condition = counts_ref['study_conditions'].value_counts().index.tolist()\n",
    "            condition = condition[0].split(';')\n",
    "\n",
    "            # time point T_0\n",
    "            t_0_index = np.array([i for i in range(len(condition)) if condition[i] in curr_conditions[0]])\n",
    "            # time point T_end\n",
    "            t_end_index = np.array([i for i in range(len(condition)) if condition[i] in curr_conditions[1]])\n",
    "\n",
    "            # get counts\n",
    "            replicate_sep =  counts_ref[\"count_replicates\"].apply(    \n",
    "                lambda x: np.array(x.split(\";\"), dtype = np.float64)\n",
    "            )\n",
    "\n",
    "            # get time point \n",
    "            t_0_comb = replicate_sep.apply(\n",
    "                lambda x: check_repeated_constructs(x, t_0_index)\n",
    "            ).apply(lambda x: ';'.join(x.astype(np.str_)))\n",
    "\n",
    "            t_end_comb = replicate_sep.apply(\n",
    "                lambda x: check_repeated_constructs(x, t_end_index)#np.median(x[t_end_index])\n",
    "            ).apply(lambda x: ';'.join(x.astype(np.str_)))\n",
    "\n",
    "            counts_ref['T0_counts'] = t_0_comb\n",
    "            counts_ref['T0_replicate_names'] = ';'.join(curr_conditions[0])\n",
    "            counts_ref['TEnd_counts'] = t_end_comb\n",
    "            counts_ref['TEnd_replicate_names'] = ';'.join(curr_conditions[1])\n",
    "\n",
    "\n",
    "        # proceed to add the orientation\n",
    "\n",
    "        unsorted_orientations = np.array(['|'.join([counts_ref[\"gene_1\"].iloc[i], counts_ref[\"gene_2\"].iloc[i]]) for i in range(counts_ref.shape[0])])\n",
    "        sorted_orientations = np.array(['|'.join(sorted([counts_ref[\"gene_1\"].iloc[i], counts_ref[\"gene_2\"].iloc[i]])) for i in range(counts_ref.shape[0])])\n",
    "\n",
    "        counts_ref['gene_pair'] = sorted_orientations\n",
    "        counts_ref['gene_pair_orientation'] = 'A_B'\n",
    "        counts_ref['gene_pair_orientation'].loc[sorted_orientations != unsorted_orientations] = 'B_A'\n",
    "        \n",
    "        counts_ref_list = []\n",
    "        # applied for HORLBECK\n",
    "        if remove_unrelated_counts:\n",
    "            print('remove_unrelated_counts = TRUE')\n",
    "            counts_ref_list = []\n",
    "            for cl in sorted(list(set(counts_ref['cell_line_origin_origin']))):\n",
    "                print('For cl = ' + cl)\n",
    "                temp_counts = counts_ref.loc[counts_ref['cell_line_origin_origin'] == cl]\n",
    "                temp_scores = score_ref.loc[score_ref['cell_line_origin_origin'] == cl]\n",
    "                \n",
    "                # get score genes + control\n",
    "                score_genes = list(set(temp_scores['gene_1'].tolist() + temp_scores['gene_2'].tolist())) + study_controls\n",
    "                if study_controls is not None:\n",
    "                    score_genes = score_genes + study_controls\n",
    "                # filter down\n",
    "                temp_counts_filt = temp_counts.loc[(temp_counts['gene_1'].isin(score_genes) & temp_counts['gene_2'].isin(score_genes)) | (temp_counts['target_type'] == 'Control')]\n",
    "                \n",
    "                removed = temp_counts.shape[0] - temp_counts_filt.shape[0]\n",
    "                if removed != 0:\n",
    "                    print('Removed a total of {rem} sgRNAs...'.format(rem = removed))\n",
    "                else:\n",
    "                    print('No unrelated counts found!')\n",
    "                    \n",
    "                counts_ref_list.append(temp_counts_filt)\n",
    "                \n",
    "            counts_ref = pd.concat(counts_ref_list, axis = 0)\n",
    "            counts_ref.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    ################################# counts ref - DONE\n",
    "    \n",
    "    print('Sequence reference...')\n",
    "    if sequence_ref is not None:\n",
    "        for col in ['sgRNA_guide_name', 'sgRNA_guide_seq', 'sgRNA_target_name']:\n",
    "            sequence_ref[col] = [i.upper() for i in sequence_ref[col]]\n",
    "            \n",
    "        # set the target names to control\n",
    "        control_idx = np.array([True if str(sequence_ref['sgRNA_target_name'].iloc[i]) in study_controls else False for i in range(sequence_ref.shape[0])]) | np.array([True if str(sequence_ref['sgRNA_guide_name'].iloc[i]) in study_controls else False for i in range(sequence_ref.shape[0])])\n",
    "        sequence_ref.loc[control_idx, 'sgRNA_target_name'] = 'CONTROL'    \n",
    "        # add study origin as well\n",
    "        sequence_ref['study_origin'] = [counts_ref['study_origin'].iloc[0]] * sequence_ref.shape[0]\n",
    "        \n",
    "        sequence_ref.reset_index(drop=True, inplace = True)\n",
    "    \n",
    "    ################################# sequence ref - DONE\n",
    "    \n",
    "    print('Done! Returning...')\n",
    "    return({'sequence_ref': sequence_ref,\n",
    "            'counts_ref': counts_ref,\n",
    "            'score_ref': score_ref})\n",
    "\n",
    "def sort_pairs_and_guides(curr_counts):\n",
    "    # sort the genes and guides based on gene ordering\n",
    "    print('Sorting gene pairs and guides based on ordering gene ordering...')\n",
    "    gene_pairs = []\n",
    "    gene_pair_guides = []\n",
    "    for i in range(curr_counts.shape[0]):\n",
    "\n",
    "        guide_1 = curr_counts['sgRNA_guide_name_g1'].iloc[i]\n",
    "        guide_2 = curr_counts['sgRNA_guide_name_g2'].iloc[i]\n",
    "\n",
    "        gene_1 = curr_counts['sgRNA_target_name_g1'].iloc[i]\n",
    "        gene_2 = curr_counts['sgRNA_target_name_g2'].iloc[i]\n",
    "\n",
    "        t_gene_1, t_gene_2 = sorted([gene_1, gene_2])\n",
    "\n",
    "\n",
    "        if (t_gene_1 == gene_1) and (t_gene_2 == gene_2):\n",
    "            gene_1 = t_gene_1\n",
    "            gene_2 = t_gene_2\n",
    "        else:\n",
    "            gene_1 = t_gene_1\n",
    "            gene_2 = t_gene_2\n",
    "\n",
    "            # swap the guides accordingly\n",
    "            temp = guide_1\n",
    "            guide_1 = guide_2\n",
    "            guide_2 = temp\n",
    "\n",
    "        gene_pairs.append('|'.join([gene_1, gene_2]))\n",
    "        gene_pair_guides.append('|'.join([guide_1, guide_2]))\n",
    "\n",
    "    return(gene_pairs, gene_pair_guides)\n",
    "\n",
    "def reindex_alphbetically(df):\n",
    "    result = []\n",
    "    for index, row in df.iterrows():\n",
    "        a, b = index.split('_')\n",
    "        if a < b:\n",
    "            result.append(f'{a}_{b}')\n",
    "        else:\n",
    "            result.append(f'{b}_{a}')\n",
    "    \n",
    "    \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e509d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d065afa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hamda\\OneDrive\\Documents\\GitHub\\PostDoc\\Conway\\Research\\GIScoring\\Hamda's Work\\Horlbeck\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
